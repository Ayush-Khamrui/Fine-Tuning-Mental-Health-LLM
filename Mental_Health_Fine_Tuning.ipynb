{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyN11Sckvm5T9ts2b9kM8AP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayush-Khamrui/Fine-Tuning-Mental-Health-LLM/blob/main/Mental_Health_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mental Health EDA and Fine-Tuning Visualizations"
      ],
      "metadata": {
        "id": "VanDsEkVt-vK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective:**\n",
        "\n",
        "Using a synthetic mental health dataset (10,000 records) we will fine-tune microsoft/Phi-3.5-mini-instruct model, and also create impactful, futuristic visualizations to communicate insights.\n",
        "\n",
        "This comprehensive script covers the entire workflow from Exploratory Data Analysis (EDA) of a synthetic mental health records dataset to data preprocessing, prompt engineering for a classification + generation task, and finally fine-tuning a large language model (Microsoft's Phi-3.5-mini-instruct) using Hugging Face Transformers and PEFT/LoRA. We also include model evaluation examples and discuss deployment options (Hugging Face Spaces and FastAPI).\n"
      ],
      "metadata": {
        "id": "t73fHiVxuDfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Exploratory Data Analysis (EDA)\n",
        "First, we load the dataset (mental_health_data.csv) and perform initial exploration. The dataset contains 10,000 synthetic records with fields: Age, Gender, Symptoms, Therapy History, Medication, Depression Score, Anxiety Score, General Well-Being Score. We will use pandas to inspect the data structure, check for missing values, and understand basic statistics and distributions."
      ],
      "metadata": {
        "id": "Cmts7CNrv-mE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the synthetic mental health dataset\n",
        "df = pd.read_csv(\"/content/mental_health_dataset.csv\")\n",
        "\n",
        "# Preview the first few rows\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(df.head(5))"
      ],
      "metadata": {
        "id": "Cz7hGh5swT0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The dataset has 10,000 records and 7 columns. Each row represents a patient's record.\n",
        "*   Age is numerical, Gender is categorical (e.g., Male/Female). Symptoms might be a text description or list of symptoms, Therapy History and Medication are textual (possibly indicating if the person had therapy before or currently on medication), and the last three fields are numeric mental health scores (Depression, Anxiety, Well-Being)."
      ],
      "metadata": {
        "id": "zV9e0YZzwg5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's perform some basic EDA to understand the data distribution:"
      ],
      "metadata": {
        "id": "pCY07L9uxbJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic summary statistics for numeric columns\n",
        "print(df.describe())\n",
        "\n",
        "# Check for any missing values in each column\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Distribution of categorical fields\n",
        "print(\"\\nGender distribution:\\n\", df['Gender'].value_counts())\n",
        "\n",
        "# If Symptoms is textual, see a sample of unique symptom descriptions\n",
        "print(\"\\nSample Symptoms:\\n\", df['Symptoms'].head(3).tolist())"
      ],
      "metadata": {
        "id": "K9HMA3s8xYH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will look at key insights:\n",
        "\n",
        "Age Distribution: Check mean, min, max age. If needed, plot a histogram to see the age distribution.\n",
        "\n",
        "Gender Distribution: Count of records by gender.\n",
        "\n",
        "Score Distributions: Are Depression/Anxiety/Well-being scores on a similar scale? Compute means and quartiles.\n",
        "\n",
        "Correlations: Quick check if depression and anxiety scores are correlated, etc."
      ],
      "metadata": {
        "id": "eOdb_9gMyqGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For deeper insight, compute correlation between numeric scores\n",
        "numeric_cols = ['Depression Score', 'Anxiety Score', 'General Well‑Being Score']\n",
        "print(\"\\nCorrelation matrix (Pearson) among scores:\\n\", df[numeric_cols].corr())"
      ],
      "metadata": {
        "id": "S0rPaPmryvyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key EDA Take-aways (Correlation Matrix)\n",
        "\n",
        "| Pair of Metrics | Pearson *r* | Interpretation |\n",
        "|-----------------|-------------|----------------|\n",
        "| **Depression ↔ Anxiety** | **+0.87** | Very strong positive link → higher depression accompanies higher anxiety. |\n",
        "| **Depression ↔ Well-Being** | **–0.97** | Near-perfect negative link → as depression rises, well-being plummets. |\n",
        "| **Anxiety ↔ Well-Being** | **–0.94** | Very strong negative link → higher anxiety correlates with lower well-being. |\n",
        "\n",
        "\n",
        "#### What this means\n",
        "\n",
        "1. **Co-morbidity:** Depression & anxiety strongly co-occur—integrated interventions are essential.  \n",
        "2. **Well-Being = Reverse Distress Index:** A single well-being KPI can reliably flag high-risk cases.  \n",
        "3. **Multicollinearity Warning:** Extreme correlations suggest dropping or regularising redundant variables in predictive models.  \n",
        "4. **Synthetic-Data Check:** Such near-deterministic links signal the generator might need extra noise for greater realism.  \n",
        "5. **Executive KPI:** Track “% of population with low well-being” as a headline metric—it mirrors depression/anxiety burden.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gAmo9N7P1KRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preprocessing\n",
        "With an understanding of the data, we proceed to preprocessing. This involves cleaning and transforming the data into a form suitable for model training:\n",
        "\n",
        "The Very first step would have been to Handling Missing Data if any missing values were found in EDA and then decide how to handle them (e.g., drop or impute). But as for our data Doesn't have any missing values so we will skip this step. If at any later stage we plan to go forward with any other dataset having missing values we can use the following code.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Drop records with any missing values (if any)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "```\n",
        "Note - If the model needed numeric encoding for non-text tasks, we'd one-hot encode or label encode, but here we plan to feed text directly to a language model. We simply ensure consistency and as per our previous EDA our data is consistent.\n",
        "\n",
        "For free-text fields like Symptoms or Therapy History, we want to clean them (remove problematic characters, trim whitespace). If Symptoms are stored as lists or comma-separated strings, join them into a single descriptive string.\n"
      ],
      "metadata": {
        "id": "gN4mSNw02bDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure symptoms are a single string of description\n",
        "df['Symptoms'] = df['Symptoms'].astype(str)  # make sure it's string\n",
        "df['Symptoms'] = df['Symptoms'].str.replace(r'[\\[\\]\\'\"]', '', regex=True)  # remove brackets/quotes if any"
      ],
      "metadata": {
        "id": "PlczdFgU2e84"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the score ranges differ greatly or use different scales (say Depression Score 0–30 vs Well-Being 0–100), Normalization could help to scale them for consistency. Normalizing can help if we were feeding numbers into certain models directly. In our case, since we will convert everything to text, scaling is not strictly necessary for the language model. However, for completeness, we demonstrate scaling the scores between 0 and 1 using Min-Max normalization:"
      ],
      "metadata": {
        "id": "P06QB4uE5WtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[['Depression Score', 'Anxiety Score', 'General Well‑Being Score']] = scaler.fit_transform(\n",
        "    df[['Depression Score', 'Anxiety Score', 'General Well‑Being Score']])"
      ],
      "metadata": {
        "id": "vdWujnLa5RTL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Plotting graphs and Dashboards\n",
        "###  Score Distributions\n",
        "Understanding how mental health scores are distributed is an early step in data analysis and provides quick answers to important questions about range, central tendency, skewness, and outliers\n",
        ". We will visualize the distributions of Depression, Anxiety, and Well-Being scores. This helps us see the overall mental health landscape in the dataset (e.g., are most patients scoring high on depression or not?) and identify any unusual patterns (bimodal distributions, extreme values, etc.).\n",
        "\n",
        "Furthermore, we examine how these distributions vary across key subgroups – by gender, age group, and therapy history – to check for notable differences. For instance, do a particular age group or gender report higher depression levels? Does therapy history relate to lower anxiety scores? Such subgroup analysis can highlight disparities and inform targeted interventions.\n",
        "\n",
        "### Distribution of Scores (Overall)\n",
        "\n",
        "First, we plot the overall distribution of each score. We use Seaborn with a modern theme for clarity. Histograms with KDE (Kernel Density Estimate) overlays will show the frequency distribution smoothed into a curve. Each score will be in a separate subplot for readability."
      ],
      "metadata": {
        "id": "vkP71k-Q6AIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set a modern style for the plots\n",
        "sns.set_theme(context=\"talk\", style=\"whitegrid\", palette=\"deep\")  # Larger font for readability\n",
        "\n",
        "# Plot histograms with KDE for each score\n",
        "scores = [\"Depression Score\", \"Anxiety Score\", \"General Well‑Being Score\"]  # Use actual column names\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, score in enumerate(scores):\n",
        "    sns.histplot(data=df, x=score, kde=True, color=sns.color_palette()[i], ax=axes[i])\n",
        "    axes[i].set_title(f\"Distribution of {score} Scores\")\n",
        "    axes[i].set_xlabel(f\"{score}\")  # No need to add 'Score' again\n",
        "    axes[i].set_ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_umuYjT5sii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of scores by Gender (overlayed KDEs)\n",
        "genders = df[\"Gender\"].unique()  # e.g., ['Male', 'Female', 'Other']\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, score in enumerate(scores):\n",
        "    sns.kdeplot(data=df, x=score, hue=\"Gender\", fill=True, common_norm=False,\n",
        "                alpha=0.5, ax=axes[i])\n",
        "    axes[i].set_title(f\"{score} Distribution by Gender\")\n",
        "    axes[i].set_xlabel(f\"{score} Score\")\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "\n",
        "    # Get the legend handles and labels\n",
        "    handles, labels = axes[i].get_legend_handles_labels()\n",
        "\n",
        "    # Create a dictionary mapping gender to color\n",
        "    color_dict = dict(zip(labels, [h.get_facecolor() for h in handles]))\n",
        "\n",
        "    # Add a text description of color mapping below the legend\n",
        "    legend_text = \"\\n\".join([f\"{gender}: {color}\" for gender, color in color_dict.items()])\n",
        "    axes[i].text(0.95, 0.05, legend_text, transform=axes[i].transAxes, ha=\"right\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6GchClwp7Sll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create age group categories (example bins)\n",
        "df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 24, 39, 59, 120],\n",
        "                        labels=['<25', '25-39', '40-59', '60+'])\n",
        "\n",
        "# Distribution of Depression, Anxiety, WellBeing by Age Group\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, score in enumerate(scores):\n",
        "    sns.histplot(data=df, x=score, hue=\"AgeGroup\", element=\"step\", stat=\"density\", common_norm=False, ax=axes[i])\n",
        "    axes[i].set_title(f\"{score} by Age Group\")\n",
        "    axes[i].set_xlabel(f\"{score} Score\")\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "\n",
        "    # Get legend handles and labels\n",
        "    handles, labels = axes[i].get_legend_handles_labels()\n",
        "\n",
        "    # Create color dictionary for age groups\n",
        "    color_dict = dict(zip(labels, [h.get_edgecolor() for h in handles])) # get_edgecolor for step plot\n",
        "\n",
        "    # Add text description below the legend\n",
        "    legend_text = \"\\n\".join([f\"{age_group}: {color}\" for age_group, color in color_dict.items()])\n",
        "    axes[i].text(0.95, 0.05, legend_text, transform=axes[i].transAxes, ha=\"right\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PgBdnMMF9cLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Therapy History' is a boolean or categorical column (e.g., 'Yes'/'No')\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for i, score in enumerate(scores):\n",
        "    sns.kdeplot(data=df, x=score, hue=\"Therapy History\", fill=True, common_norm=False, ax=axes[i])\n",
        "    axes[i].set_title(f\"{score} by Therapy History\")\n",
        "    axes[i].set_xlabel(f\"{score} Score\")\n",
        "    axes[i].set_ylabel(\"Density\")\n",
        "\n",
        "    # Get legend handles and labels\n",
        "    handles, labels = axes[i].get_legend_handles_labels()\n",
        "\n",
        "    # Create color dictionary for Therapy History\n",
        "    color_dict = dict(zip(labels, [h.get_facecolor() for h in handles]))\n",
        "\n",
        "    # Add text description below the legend\n",
        "    legend_text = \"\\n\".join([f\"{therapy_history}: {color}\" for therapy_history, color in color_dict.items()])\n",
        "    axes[i].text(0.95, 0.05, legend_text, transform=axes[i].transAxes, ha=\"right\", va=\"bottom\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_NHXEN4C-D0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate KPI values\n",
        "avg_dep = df[\"Depression Score\"].mean()\n",
        "avg_anx = df[\"Anxiety Score\"].mean()\n",
        "avg_wb = df[\"General Well‑Being Score\"].mean()\n",
        "\n",
        "# Define \"high-risk\" as Depression Score > 0.75 and General Well‑Being Score < 0.40\n",
        "# (Adjusted for normalized scores between 0 and 1)\n",
        "high_risk = df[(df[\"Depression Score\"] > 0.75) & (df[\"General Well‑Being Score\"] < 0.40)]\n",
        "high_risk_pct = len(high_risk) / len(df) * 100  # percentage of high-risk patients\n",
        "\n",
        "# Therapy effectiveness: compare average well-being of those with therapy vs without\n",
        "therapy_group_wb = df[df[\"Therapy History\"] == \"Yes\"][\"General Well‑Being Score\"].mean()\n",
        "no_therapy_group_wb = df[df[\"Therapy History\"] == \"No\"][\"General Well‑Being Score\"].mean()\n",
        "wb_diff = therapy_group_wb - no_therapy_group_wb  # difference in well-being\n",
        "\n",
        "# Print the calculated values (to get output)\n",
        "print(f\"Average Depression Score: {avg_dep}\")\n",
        "print(f\"Average Anxiety Score: {avg_anx}\")\n",
        "print(f\"Average Well-Being Score: {avg_wb}\")\n",
        "print(f\"Percentage of High-Risk Patients: {high_risk_pct:.2f}%\")\n",
        "print(f\"Difference in Well-Being (Therapy vs No Therapy): {wb_diff}\")"
      ],
      "metadata": {
        "id": "2HWwswpqCS-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud matplotlib  # install the wordcloud library if not already\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# Combine all symptom text into one string\n",
        "all_symptoms_text = \" \".join(df[\"Symptoms\"])  # assuming 'Symptoms' column contains text descriptions\n",
        "\n",
        "# Optional: define stopwords to exclude common irrelevant words (if any domain-specific ones needed)\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.update([\"patient\", \"feel\"])  # example: remove generic words that might appear often but carry little info\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", stopwords=stopwords, colormap=\"viridis\")\n",
        "wordcloud.generate(all_symptoms_text)\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Most Common Symptoms Word Cloud\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cb3DUZ7aDZVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split\n",
        "Before converting to prompt format, it's good practice to split our data into training and testing sets. This allows us to fine-tune on training data and later evaluate on unseen data."
      ],
      "metadata": {
        "id": "dlAdAyqz1VtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(\"Training samples:\", len(train_df), \"| Testing samples:\", len(test_df))"
      ],
      "metadata": {
        "id": "N8JFG6jizili"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Prompt-Completion Formatting for Instruction Fine-Tuning\n",
        "The goal is to fine-tune a language model so that it can accept a few survey-style inputs and generate a mental health status summary or recommendation. This means we need to convert each record into a prompt (input) and a completion (desired output) pair suitable for an instruction-following fine-tuning.\n",
        "\n",
        "Designing the Prompt: We will include key fields (Age, Gender, Symptoms, etc.) as the context given to the model. Since the model is an instruct model (already tuned to follow an instruction prompt), we'll phrase the input as an instruction or question.\n",
        "\n",
        "**For example:**\n",
        "\n",
        "**Prompt example:**\n",
        "\n",
        "Given the following patient information:\n",
        "\n",
        "Age: 29\n",
        "\n",
        "Gender: Female\n",
        "\n",
        "Symptoms: \"sadness, loss of appetite, insomnia\"\n",
        "\n",
        "Therapy History: No prior therapy\n",
        "\n",
        "Medication: None\n",
        "\n",
        "Depression Score: 0.7 (normalized)\n",
        "\n",
        "Anxiety Score: 0.5 (normalized)\n",
        "\n",
        "General Well-Being Score: 0.6 (normalized)\n",
        "\n",
        "Provide a brief summary of the patient's mental health status and a recommendation.\" The above is a single input string that the model will see. We may include a clear instruction phrase like \"Provide a summary...\" to cue the model's response.\n",
        "\n",
        "Designing the Completion (Output): This should be a concise, human-readable summary or recommendation based on the input. For instance, for the above prompt, an ideal completion might be:\n",
        "\n",
        "**Completion example:**\n",
        "\n",
        "\"The patient is showing moderate symptoms of depression (given the sadness and insomnia) and mild anxiety. Their general well-being is somewhat affected. It is recommended that they consider speaking with a therapist and possibly a medical evaluation for depression treatment. Regular exercise and social support might also improve well-being.\"\n",
        "This completion includes classifications (moderate depression, mild anxiety) and a recommendation, phrased in a helpful, empathetic manner. Since our dataset likely does not come with written summaries, we will generate a pseudo-ground truth completion for training. We can use simple rules based on the numeric scores and symptoms:\n",
        "\n",
        "If Depression Score (normalized) > 0.66, label as \"high signs of depression\", 0.33–0.66 \"moderate depression\", <0.33 \"minimal/mild depression\".\n",
        "\n",
        "Similarly for Anxiety Score.\n",
        "\n",
        "Well-Being Score low might strengthen recommendations."
      ],
      "metadata": {
        "id": "cMAIh50s1lbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's create a helper function to generate a summary from a record**"
      ],
      "metadata": {
        "id": "HuB2J3te6MLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(age, gender, symptoms, therapy_hist, medication, dep_score, anx_score, well_score):\n",
        "    # Convert normalized scores to categorical descriptions for readability\n",
        "    def score_to_level(score, metric):\n",
        "        if score >= 0.67:\n",
        "            level = \"high\"\n",
        "        elif score >= 0.34:\n",
        "            level = \"moderate\"\n",
        "        else:\n",
        "            level = \"mild\"\n",
        "        # Customize wording for specific metric if needed\n",
        "        if metric == \"wellbeing\":\n",
        "            # Invert interpretation: high well_score = good well-being\n",
        "            if score >= 0.67:\n",
        "                return \"high overall well-being\"\n",
        "            elif score >= 0.34:\n",
        "                return \"moderate overall well-being\"\n",
        "            else:\n",
        "                return \"low overall well-being\"\n",
        "        return level\n",
        "\n",
        "    dep_level = score_to_level(dep_score, \"depression\")\n",
        "    anx_level = score_to_level(anx_score, \"anxiety\")\n",
        "    wellbeing_desc = score_to_level(well_score, \"wellbeing\")\n",
        "\n",
        "    summary = []\n",
        "    # Mention depression/anxiety levels\n",
        "    summary.append(f\"shows signs of {dep_level} depression and {anx_level} anxiety\")\n",
        "    # Mention well-being if relevant\n",
        "    summary.append(f\"with {wellbeing_desc}.\")\n",
        "    # If therapy history or medication is present, include in recommendation\n",
        "    if str(therapy_hist).strip().lower() in [\"none\", \"no\", \"no prior therapy\", \"nan\"]:\n",
        "        # no therapy yet\n",
        "        summary.append(\"Encourage seeking professional therapy as a next step\")\n",
        "    else:\n",
        "        summary.append(\"Continue with therapy and follow-ups as advised\")\n",
        "    # If on medication or medication is none\n",
        "    if str(medication).strip().lower() not in [\"none\", \"nan\", \"\"] :\n",
        "        summary.append(f\"and ensure medication ({medication}) is taken as prescribed.\")\n",
        "    else:\n",
        "        summary.append(\"and consider discussing medication options with a psychiatrist.\")\n",
        "    return \" \".join(summary)"
      ],
      "metadata": {
        "id": "6AP6j36u1gJZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, use this function to construct the prompt-completion pairs for each record in the training set:**"
      ],
      "metadata": {
        "id": "QYq_LRjm65L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs = []  # list of dicts with 'input' and 'output'\n",
        "for _, row in train_df.iterrows():\n",
        "    age = row['Age']; gender = row['Gender']; symptoms = row['Symptoms']\n",
        "    therapy = row['Therapy History']; medication = row['Medication']\n",
        "    dep = row['Depression Score']; anx = row['Anxiety Score']; well = row['General Well‑Being Score']\n",
        "\n",
        "    # Build the instruction prompt\n",
        "    prompt = (\n",
        "        f\"Patient details:\\n\"\n",
        "        f\"- Age: {age}\\n\"\n",
        "        f\"- Gender: {gender}\\n\"\n",
        "        f\"- Symptoms: {symptoms}\\n\"\n",
        "        f\"- Therapy History: {therapy}\\n\"\n",
        "        f\"- Medication: {medication}\\n\"\n",
        "        f\"- Depression Score: {dep:.2f}\\n\"\n",
        "        f\"- Anxiety Score: {anx:.2f}\\n\"\n",
        "        f\"- General Well-Being Score: {well:.2f}\\n\\n\"\n",
        "        f\"Based on the above, provide a brief summary of the patient's mental health status and recommendations.\"\n",
        "    )\n",
        "    # Generate a pseudo-summary as the target completion\n",
        "    completion = generate_summary(age, gender, symptoms, therapy, medication, dep, anx, well)\n",
        "    # Ensure the completion text is properly formatted (could add a prefix like \"Summary:\" if desired)\n",
        "    completion = \"Summary: \" + completion\n",
        "\n",
        "    train_pairs.append({\"input\": prompt, \"output\": completion})"
      ],
      "metadata": {
        "id": "zRL4gz-960_S"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the same for the **test set** to have evaluation pairs (though we'll not use these for training, only to evaluate the model later):"
      ],
      "metadata": {
        "id": "ZvxBovcj7NOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pairs = []\n",
        "for _, row in test_df.iterrows():\n",
        "    # ... (similar to above)\n",
        "    prompt = (f\"Patient details:\\n- Age: {row['Age']}\\n- Gender: {row['Gender']}\\n\"\n",
        "              f\"- Symptoms: {row['Symptoms']}\\n- Therapy History: {row['Therapy History']}\\n\"\n",
        "              f\"- Medication: {row['Medication']}\\n- Depression Score: {row['Depression Score']:.2f}\\n\"\n",
        "              f\"- Anxiety Score: {row['Anxiety Score']:.2f}\\n- General Well-Being Score: {row['General Well‑Being Score']:.2f}\\n\\n\"\n",
        "              f\"Provide a brief summary of the patient's mental health status and recommendation.\")\n",
        "    completion = generate_summary(row['Age'], row['Gender'], row['Symptoms'],\n",
        "                                  row['Therapy History'], row['Medication'],\n",
        "                                  row['Depression Score'], row['Anxiety Score'], row['General Well‑Being Score'])\n",
        "    completion = \"Summary: \" + completion\n",
        "    test_pairs.append({\"input\": prompt, \"output\": completion})"
      ],
      "metadata": {
        "id": "w_2hpyD6696j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have our data in the **instruction fine-tuning format**: each item in train_pairs and test_pairs is a dictionary with an \"input\" (the prompt instruction) and an \"output\" (the expected model response). This format will be used to fine-tune the language model.\n",
        "\n",
        "Let's inspect one example pair to ensure it looks correct:"
      ],
      "metadata": {
        "id": "O54bO4MV7e4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example Prompt:\\n\", train_pairs[0]['input'])\n",
        "print(\"\\nExample Expected Response:\\n\", train_pairs[0]['output'])"
      ],
      "metadata": {
        "id": "QlN80OX_7UVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This confirms that our prompt and target summary are properly constructed."
      ],
      "metadata": {
        "id": "qdqNrHQb7_yj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Fine-Tuning the 'microsoft/Phi-3.5-mini-instruct' Model with LoRA\n",
        "\n",
        "Now we proceed to fine-tune the large language model. We use **Hugging Face Transformers** to load the pre-trained microsoft/Phi-3.5-mini-instruct model, and apply **PEFT (Parameter-Efficient Fine-Tuning) ** via **LoRA (Low-Rank Adaptation)** to keep the training feasible on Colab (which typically has a single GPU with ~16GB VRAM or less).\n",
        "\n",
        "**Why LoRA?** LoRA allows us to fine-tune a model by introducing a small number of trainable parameters (low-rank matrices) into each layer **instead of updating all 350 million+ parameters** of the model\n",
        ". This greatly reduces memory usage and training time while still effectively adapting the model to our task.\n",
        "\n",
        "**Model Loading and Configuration:** We'll load the model in 4-bit precision to further save memory, and then wrap it with LoRA adapters."
      ],
      "metadata": {
        "id": "UpqDR8Kx8y7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== 1. Install / upgrade libraries =====================\n",
        "!pip install -qU transformers accelerate peft bitsandbytes datasets\n",
        "\n",
        "# ===================== 2. Imports =====================\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "\n",
        "# ===================== 3. Model & tokenizer names =====================\n",
        "base_model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "# ===================== 4. 4-bit quantisation config =====================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"      # use bfloat16 on A100/H100 if available\n",
        ")\n",
        "\n",
        "# ===================== 5. Tokenizer =====================\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id   # important for batching\n",
        "\n",
        "# ===================== 6. Load model (4-bit) =====================\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,            # computation type\n",
        ")\n",
        "torch.cuda.empty_cache()                 # clear any residual memory\n",
        "\n",
        "# OPTIONAL: Gradient checkpointing to save RAM during training\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# ===================== 7. Prep for k-bit training =====================\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# ===================== 8. LoRA config & injection =====================\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# ===================== 9. Check that only LoRA adapters are trainable =====================\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ===================== 10. (Optional) quick sanity-check forward pass =====================\n",
        "text = \"Once upon a time,\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ky0ASfC-7tp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above:\n",
        "\n",
        "*   We installed required packages: transformers, accelerate, peft, bitsandbytes, datasets\n",
        "*   Loaded the Phi-3.5-mini model in 4-bit precision to fit in memory\n",
        ". This model is a smaller variant of Phi-3.5 (likely around 1.3B parameters) and is instruction-tuned by Microsoft for general tasks.\n",
        "\n",
        "*   Configured LoRA with a rank of 16 (a small rank for Colab) and applied it to all key linear layers in the model (query, key, value, output projections in self-attention, and the gating/up/down in the feedforward layers, which we determined by inspecting model architecture or references\n",
        "*   We print trainable parameters to verify that only the LoRA layers (and possibly embeddings if not frozen) are trainable. This should show a small fraction of total parameters (e.g., \"Trainable params: 0.2M out of 350M\" etc.).\n",
        "\n",
        "**Preparing the Dataset for Training:** We have train_pairs and test_pairs as lists of dictionaries. We can convert them into a Hugging Face Dataset object for convenience:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NchRCqmABsAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_list(train_pairs)\n",
        "test_dataset = Dataset.from_list(test_pairs)\n",
        "\n",
        "# We have 'input' and 'output' fields. We will need to concatenate them for the model.\n",
        "print(train_dataset.features)\n",
        "# Expected output: {'input': Value(dtype='string', ...), 'output': Value(dtype='string', ...)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Hoz-I8-i6U",
        "outputId": "ba61adfd-d7b0-4936-e8ea-f4296d3a5a1f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization and Data Collation:** For causal language modeling fine-tuning, we need to tokenize the prompt and completion and create labels such that:\n",
        "\n",
        "*   The model is **given the full prompt and the completion as input**.\n",
        "*   The **loss is computed only on the completion part** (the model should not be penalized for \"predicting\" the prompt, which is given).\n",
        "\n",
        "We achieve this by constructing the input sequence as **[PROMPT] [COMPLETION]**, and then masking the prompt tokens in the labels (set them to -100 so that loss is only calculated on the completion tokens). We'll implement a custom data collator to handle this:\n"
      ],
      "metadata": {
        "id": "80--qjzyCtSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "class PromptCompletionCollator:\n",
        "    def __init__(self, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        # Each example is a dict with 'input' and 'output' keys\n",
        "        prompts = [ex[\"input\"] for ex in examples]\n",
        "        completions = [ex[\"output\"] for ex in examples]\n",
        "        # Append the completion to the prompt with a separator (here just a space and ensure EOS at end)\n",
        "        # We include the end-of-sequence token at the end of completion.\n",
        "        combined = [p + \"\\n\" + c + tokenizer.eos_token for p, c in zip(prompts, completions)]\n",
        "        tokenized = tokenizer(combined, truncation=True, padding=\"longest\", max_length=self.max_length, return_tensors=\"pt\")\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # Create labels, with prompt part masked as -100, ensuring it requires grad\n",
        "        # labels = input_ids.clone().detach() # detach then requires_grad_() - This is the problematic line\n",
        "        # labels.requires_grad = True # This ensures it requires grad for computations - Remove this line as its not required\n",
        "        # The line below fixes the issue. No need to explicitly set requires_grad_()\n",
        "        labels = input_ids.clone()\n",
        "        # For each example, find prompt length and mask it out in labels\n",
        "        for i in range(len(examples)):\n",
        "            prompt_tokens = tokenizer(prompts[i], truncation=True, max_length=self.max_length, add_special_tokens=False).input_ids\n",
        "            # label prompt tokens as -100\n",
        "            prompt_len = len(prompt_tokens)\n",
        "            if prompt_len > self.max_length:\n",
        "                prompt_len = self.max_length  # in case prompt itself is longer than max (rare)\n",
        "            labels[i, :prompt_len] = -100  # mask prompt portion\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# Instantiate our collator\n",
        "data_collator = PromptCompletionCollator(tokenizer, max_length=512)"
      ],
      "metadata": {
        "id": "imExCsWQCo8p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set a max_length of 512 tokens for training (adjust as needed based on prompt size; 512 is reasonable for our short records). Now our data and model are ready for training.\n",
        "\n",
        "**Training Hyperparameters:** We choose training parameters that are reasonable for Colab:\n",
        "*   A small batch size (we will use batch size 1 or 2, and use gradient accumulation to effectively have a larger batch).\n",
        "*   A few epochs (e.g., 2-3 epochs over 8k training examples might be enough given the data size).\n",
        "*   A learning rate suitable for fine-tuning a language model with LoRA. The QLoRA paper and community examples suggest on the order of 2e-5 to 2e-4 for such scenarios. We'll pick, say, 2e-4 here since our dataset is not too large.\n",
        "*   Gradient checkpointing to reduce memory, if supported.\n",
        "*   We also disable unnecessary logging or evaluation during training for speed.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TPXbMRq7D6NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "output_dir = \"phi-mentalhealth-model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,   # small batch per device\n",
        "    gradient_accumulation_steps=4,   # accumulate to simulate batch of 2*4 = 8\n",
        "    learning_rate=2e-4,             # learning rate (could tune between 2e-5 and 2e-4)\n",
        "    lr_scheduler_type=\"cosine\",     # cosine schedule (common for fine-tuning)\n",
        "    warmup_ratio=0.03,              # 3% of steps for warmup:contentReference[oaicite:9]{index=9}\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    fp16=True,                      # use 16-bit training if available\n",
        "    gradient_checkpointing=True,    # save memory\n",
        "    # evaluation_strategy=\"no\",    # Removed this argument\n",
        "    # Use the report_to argument instead for older versions of transformers\n",
        "    report_to=\"none\",                # Disable reporting (equivalent to evaluation_strategy=\"no\")\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,  # Add this line to prevent the Trainer from removing unused columns\n",
        ")"
      ],
      "metadata": {
        "id": "ws8fd_MFDzYP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we initialize the Hugging Face **Trainer** with our model, data, and training configurations:"
      ],
      "metadata": {
        "id": "AI5HPXizEuNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "dMDLw2ckEcTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is set. Start **fine-tuning:**"
      ],
      "metadata": {
        "id": "SS6PX1W-E-do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "hvm6jZJ-E4tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, we expect to see the loss decreasing gradually. With LoRA, only a small number of weights are updated, which is efficient. (If using Colab's free GPU, training ~8k examples for a few epochs might take around 10-20 minutes depending on the GPU.)\n",
        "\n",
        "After training completes, our model now has the fine-tuned LoRA weights. We should save the fine-tuned model (including LoRA adapters). The Trainer will automatically save the final model to output_dir (with LoRA modules). We can also push this model to the Hugging Face Hub or save to Google Drive if needed."
      ],
      "metadata": {
        "id": "7kGekjG_qJ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀  MERGE LoRA into Phi-3.5-mini (A100) AFTER trainer.train()\n",
        "\n",
        "!pip install -U peft==0.8.2 transformers accelerate bitsandbytes -q\n",
        "\n",
        "import torch, gc, os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ── 1️⃣ Paths ─────────────────────────────────────────────────────────────────────\n",
        "BASE_MODEL  = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "ADAPTER_DIR = training_args.output_dir        # folder where Trainer saved the LoRA\n",
        "MERGED_DIR  = ADAPTER_DIR + \"_merged\"\n",
        "\n",
        "# If the adapter hasn’t been written yet, save it now\n",
        "trainer.model.save_pretrained(ADAPTER_DIR)\n",
        "print(f\"Adapter at: {ADAPTER_DIR}\")\n",
        "\n",
        "# ── 2️⃣ Free GPU RAM (optional but tidy) ──────────────────────────────────────────\n",
        "del trainer\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# ── 3️⃣ Load base model in fp16 on the A100 ───────────────────────────────────────\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0}          # put on cuda:0\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# ── 4️⃣ Attach LoRA adapter ───────────────────────────────────────────────────────\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_DIR, device_map={\"\": 0})\n",
        "\n",
        "# ── 5️⃣ Merge & unload  ➜ returns plain HuggingFace model (fp16) ─────────────────\n",
        "model = model.merge_and_unload()\n",
        "model = model.half()            # ensure fp16\n",
        "\n",
        "# ── 6️⃣ Save merged model (safetensors) ───────────────────────────────────────────\n",
        "os.makedirs(MERGED_DIR, exist_ok=True)\n",
        "model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "print(f\"✅  Merged model saved to ➜ {MERGED_DIR}\")\n"
      ],
      "metadata": {
        "id": "6RB8dJFXFKcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs  = tok(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "prompt_len = inputs.input_ids.shape[-1]          # tensor, so `.shape` works\n",
        "\n",
        "out_ids = model.generate(**inputs, max_new_tokens=80)\n",
        "new_tokens = out_ids[0][prompt_len:]             # tensor slice\n",
        "print(tok.decode(new_tokens, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "dpuHquEzqTVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluating the Fine-Tuned Model\n",
        "Let's test the fine-tuned model on a few examples to ensure it generates reasonable summaries. We will use some prompts from our test_pairs (which the model has not seen during training) and compare the model's output to the expected summary.\n",
        "\n",
        "We'll use the merged_model for generation (for simplicity, as it behaves like a normal model), but we could also use the model with LoRA by keeping the peft environment if we didn't merge."
      ],
      "metadata": {
        "id": "xxXuSn-0rbYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 TEST the merged Phi-3.5-mini mental-health model (A100)\n",
        "import torch, pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_DIR = \"phi-mentalhealth-model_merged\"   # ← change if needed\n",
        "\n",
        "# 1️⃣  Load model & tokenizer (fp16 on GPU)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "tok.pad_token_id = tok.eos_token_id   # safety\n",
        "\n",
        "# 2️⃣  Helper to build prompt & generate summary\n",
        "def generate_summary(patient_dict,\n",
        "                     max_new_tokens=100,\n",
        "                     **gen_kwargs):\n",
        "    \"\"\"\n",
        "    patient_dict must contain keys:\n",
        "      Age, Gender, Symptoms, TherapyHistory, Medication,\n",
        "      Depression, Anxiety, WellBeing\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Patient details:\\n\"\n",
        "        f\"- Age: {patient_dict['Age']}\\n\"\n",
        "        f\"- Gender: {patient_dict['Gender']}\\n\"\n",
        "        f\"- Symptoms: {patient_dict['Symptoms']}\\n\"\n",
        "        f\"- Therapy History: {patient_dict['TherapyHistory']}\\n\"\n",
        "        f\"- Medication: {patient_dict['Medication']}\\n\"\n",
        "        f\"- Depression Score: {patient_dict['Depression']:.2f}\\n\"\n",
        "        f\"- Anxiety Score: {patient_dict['Anxiety']:.2f}\\n\"\n",
        "        f\"- General Well-Being Score: {patient_dict['WellBeing']:.2f}\\n\\n\"\n",
        "        \"Provide a brief summary of the patient's mental health status and recommendations.\"\n",
        "    )\n",
        "    # Encode & generate\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    out_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        **gen_kwargs\n",
        "    )\n",
        "    # Slice away the prompt\n",
        "    generated = out_ids[0][inputs.input_ids.shape[-1]:]\n",
        "    return tok.decode(generated, skip_special_tokens=True).strip()\n",
        "\n",
        "# 3️⃣  Run three demo cases ───────────────────────────────────────────────────\n",
        "demo1 = {\n",
        "    \"Age\": 30,\n",
        "    \"Gender\": \"Male\",\n",
        "    \"Symptoms\": \"feeling down, insomnia, loss of appetite\",\n",
        "    \"TherapyHistory\": \"No\",\n",
        "    \"Medication\": \"None\",\n",
        "    \"Depression\": 0.72,   # assuming you normalised to 0-1\n",
        "    \"Anxiety\":    0.45,\n",
        "    \"WellBeing\":  0.30\n",
        "}\n",
        "print(\"Demo 1 ➜\", generate_summary(demo1), \"\\n\")\n",
        "\n",
        "# Pull two real rows from the original CSV (if still loaded)\n",
        "try:\n",
        "    df  # if you still have the DataFrame in RAM\n",
        "except NameError:\n",
        "    df = pd.read_csv(\"/mnt/data/mental_health_dataset.csv\")  # update path if elsewhere\n",
        "\n",
        "for i, row in df.sample(2, random_state=42).iterrows():\n",
        "    sample = {\n",
        "        \"Age\": row[\"Age\"],\n",
        "        \"Gender\": row[\"Gender\"],\n",
        "        \"Symptoms\": row[\"Symptoms\"],\n",
        "        \"TherapyHistory\": row[\"Therapy History\"],\n",
        "        \"Medication\": row[\"Medication\"],\n",
        "        \"Depression\": row[\"Depression Score\"],\n",
        "        \"Anxiety\": row[\"Anxiety Score\"],\n",
        "        \"WellBeing\": row[\"General Well‑Being Score\"],\n",
        "    }\n",
        "    print(f\"CSV row {i} ➜\", generate_summary(sample), \"\\n\")\n",
        "\n",
        "# 4️⃣  (Optional) quick wrapper for any pandas row ↦ summary\n",
        "def summary_from_row(row):\n",
        "    pdict = {\n",
        "        \"Age\": row[\"Age\"], \"Gender\": row[\"Gender\"], \"Symptoms\": row[\"Symptoms\"],\n",
        "        \"TherapyHistory\": row[\"Therapy History\"], \"Medication\": row[\"Medication\"],\n",
        "        \"Depression\": row[\"Depression Score\"], \"Anxiety\": row[\"Anxiety Score\"],\n",
        "        \"WellBeing\": row[\"General Well‑Being Score\"]\n",
        "    }\n",
        "    return generate_summary(pdict)\n",
        "\n",
        "# Example usage on the first 5 test rows\n",
        "print(df.head(5).apply(summary_from_row, axis=1).tolist())"
      ],
      "metadata": {
        "id": "XwlwW0uSu0uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics Visualization after Fine-Tuned Model Creation"
      ],
      "metadata": {
        "id": "o_gX3RUbB2a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, json, math, inspect\n",
        "\n",
        "# === CONFIG ===\n",
        "MODEL_DIR = \"phi-mentalhealth-model_merged\"  # change if your merged model lives elsewhere\n",
        "\n",
        "# --- 1. Load model on CPU (weights not needed on GPU for inspection) ---\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16, device_map=\"cpu\")\n",
        "\n",
        "# --- 2. Categorise parameters ---\n",
        "categories = {\n",
        "    \"Embedding\": 0,\n",
        "    \"Attention_QKV\": 0,\n",
        "    \"Attention_Out\": 0,\n",
        "    \"FeedForward\": 0,\n",
        "    \"Other\": 0\n",
        "}\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    n = param.numel()\n",
        "    if any(k in name for k in ['embed', 'wte', 'wpe', 'embedding']):\n",
        "        categories[\"Embedding\"] += n\n",
        "    elif any(k in name for k in ['q_proj', 'k_proj', 'v_proj']):\n",
        "        categories[\"Attention_QKV\"] += n\n",
        "    elif 'o_proj' in name:\n",
        "        categories[\"Attention_Out\"] += n\n",
        "    elif any(k in name for k in ['up_proj', 'down_proj', 'gate_proj']):\n",
        "        categories[\"FeedForward\"] += n\n",
        "    else:\n",
        "        categories[\"Other\"] += n\n",
        "\n",
        "labels = list(categories.keys())\n",
        "values = np.array(list(categories.values()), dtype=float)\n",
        "\n",
        "# Convert to millions for readability\n",
        "values_m = values / 1e6\n",
        "\n",
        "# --- 3. Create a \"funky\" polar bar chart of parameter distribution ---\n",
        "plt.figure(figsize=(8, 8))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "theta = np.linspace(0.0, 2 * math.pi, len(values_m), endpoint=False)\n",
        "bars = ax.bar(theta, values_m, width=2 * math.pi / len(values_m), bottom=0.0, alpha=0.8)\n",
        "\n",
        "# Add labels\n",
        "for angle, height, label in zip(theta, values_m, labels):\n",
        "    ax.text(angle, height + max(values_m) * 0.05, f\"{label}\\n{height:.1f}M\",\n",
        "            ha='center', va='center', fontsize=10, rotation=math.degrees(angle),\n",
        "            rotation_mode='anchor')\n",
        "\n",
        "ax.set_title(\"Parameter Distribution of Merged Phi‑3.5‑mini Model\\n(all numbers in millions)\",\n",
        "             va='bottom', fontsize=14)\n",
        "ax.set_yticklabels([])  # hide radial tick labels\n",
        "ax.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_XzwmF7u1nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to HuggingFace"
      ],
      "metadata": {
        "id": "aytIl3uVBt-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install huggingface_hub gradio\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# ① paste your HF access token (with \"write\" scope)\n",
        "HF_TOKEN = \"XXXXXXXXXXXXXXX\"    #Access token with Write Access\n",
        "HfFolder.save_token(HF_TOKEN)        # stores for this runtime\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "USERNAME = api.whoami()[\"name\"]\n",
        "print(\"✔️ Logged in as:\", USERNAME)"
      ],
      "metadata": {
        "id": "vj_2X90Byt5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying the Fine-Tuned Model in HuggingFace"
      ],
      "metadata": {
        "id": "Y2R9NBmPBbZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "LOCAL_MODEL_DIR = \"phi-mentalhealth-model_merged\"   # adjust if different\n",
        "MODEL_REPO_ID   = f\"{USERNAME}/phi-mentalhealth-mini\"\n",
        "\n",
        "# Upload (skips if repo already exists & files unchanged)\n",
        "model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIR)\n",
        "tok   = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR)\n",
        "model.push_to_hub(MODEL_REPO_ID, safe_serialization=True)\n",
        "tok.push_to_hub(MODEL_REPO_ID)\n",
        "print(\"✅ Model uploaded to:\", f\"https://huggingface.co/{MODEL_REPO_ID}\")"
      ],
      "metadata": {
        "id": "TCFujuyS2_ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if the model is Successfully deployed in HuggingFace or not"
      ],
      "metadata": {
        "id": "4gMf_19CBj6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "MODEL_ID = \"AyushKhamrui/phi-mentalhealth-mini\"   # ← exact casing\n",
        "\n",
        "tok   = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=\"auto\"           # fp32 on CPU, fp16 on GPU\n",
        ")\n",
        "print(\"✓ loaded\", MODEL_ID)"
      ],
      "metadata": {
        "id": "p5i-Ys0X3gD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}